{"cells":[{"cell_type":"markdown","source":["# Importamos los paquetes necesarios"],"metadata":{"id":"wVkZCBk3Zvlc","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb00b74a-6076-4aac-a9ee-d535a1515b5e"}}},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport pyspark\nfrom pyspark.sql import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom pyspark import SparkContext, SparkConf"],"metadata":{"id":"Gy7uIqJ0ZyQd","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e25ed39-5579-423c-9e36-ba0b2338be1a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Práctica Spark ML\n\n![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n\n### Practica sobre cómo generar un flujo de ejecución en un problema de Machine Learning\n\nEsta práctica simula un ejercicio completo de ETL (Extract-Transform-Load) junto a un análisis exploratorio de un dataset real, para posteriormente aplicar differentes algoritmos de aprendizaje automático que resuelvan un problema de regresión.\n\n#### Contenido del ejercicio\n\n* *Conocimiento del dominio*\n* *Parte 1: Extracción, transformación y carga [ETL] del dataset* (2 punto sobre 10)\n* *Parte 2: Explorar los datos* (2 puntos sobre 10)\n* *Parte 3: Visualizar los datos* (2 puntos sobre 10)\n* *Parte 4: Preparar los datos* (1 puntos sobre 10)\n* *Parte 5: Modelar los datos* (3 puntos sobre 10)\n\n*Nuestro objetivo será predecir de la forma más exacta posible la energía generada por un conjunto de plantas eléctricas usando los datos generados por un conjunto de sensores.*\n\n\n## Conocimiento del dominio\n\n### Background \n\nLa generación de energía es un proceso complejo, comprenderlo para poder predecir la potencia de salida es un elemento vital en la gestión de una planta energética y su conexión a la red. Los operadores de una red eléctrica regional crean predicciones de la demanda de energía en base a la información histórica y los factores ambientales (por ejemplo, la temperatura). Luego comparan las predicciones con los recursos disponibles (por ejemplo, plantas, carbón, gas natural, nuclear, solar, eólica, hidráulica, etc). Las tecnologías de generación de energía, como la solar o la eólica, dependen en gran medida de las condiciones ambientales, pero todas las centrales eléctricas son objeto de mantenimientos tanto planificados y como puntuales debidos a un problema.\n\nEn esta practica usaremos un ejemplo del mundo real sobre la demanda prevista (en dos escalas de tiempo), la demanda real, y los recursos disponibles de la red electrica de California: http://www.caiso.com/Pages/TodaysOutlook.aspx\n\n![](http://content.caiso.com/outlook/SP/ems_small.gif)\n\nEl reto para un operador de red de energía es cómo manejar un déficit de recursos disponibles frente a la demanda real. Hay tres posibles soluciones a un déficit de energía: construir más plantas de energía base (este proceso puede costar muchos anos de planificación y construcción), comprar e importar de otras redes eléctricas regionales energía sobrante (esta opción puede ser muy cara y está limitado por las interconexiones entre las redes de transmisión de energía y el exceso de potencia disponible de otras redes), o activar pequeñas [plantas de pico](https://en.wikipedia.org/wiki/Peaking_power_plant). Debido a que los operadores de red necesitan responder con rapidez a un déficit de energía para evitar un corte del suministro, estos basan sus decisiones en una combinación de las dos últimas opciones. En esta práctica, nos centraremos en la última elección.\n\n### La lógica de negocio\n\nDebido a que la demanda de energía solo supera a la oferta ocasionalmente, la potencia suministrada por una planta de energía pico tiene un precio mucho más alto por kilovatio hora que la energía generada por las centrales eléctricas base de una red eléctrica. Una planta pico puede operar muchas horas al día, o solo unas pocas horas al año, dependiendo de la condición de la red eléctrica de la región. Debido al alto coste de la construcción de una planta de energía eficiente, si una planta pico solo va a funcionar por un tiempo corto o muy variable, no tiene sentido económico para que sea tan eficiente como una planta de energía base. Además, el equipo y los combustibles utilizados en las plantas base a menudo no son adecuados para uso en plantas de pico.\n\nLa salida de potencia de una central eléctrica pico varía dependiendo de las condiciones ambientales, por lo que el problema de negocio a resolver se podría describir como _predecir la salida de potencia de una central eléctrica pico en función de la condiciones ambientales_  - ya que esto permitiría al operador de la red hacer compensaciones económicas sobre el número de plantas pico que ha de conectar en cada momento (o si por el contrario le interesa comprar energía más cara de otra red).\n\nUna vez descrita esta lógica de negocio, primero debemos proceder a realizar un análisis exploratorio previo y trasladar el problema de negocio (predecir la potencia de salida en función de las condiciones medio ambientales) en un tarea de aprendizaje automático (ML). Por ejemplo, una tarea de ML que podríamos aplicar a este problema es la regresión, ya que tenemos un variable objetivo (dependiente) que es numérica. Para esto usaremos [Apache Spark ML Pipeline](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark-ml-package) para calcular dicha regresión.\n\nLos datos del mundo real que usaremos en esta práctica se componen de 9.568 puntos de datos, cada uno con 4 atributos ambientales recogidos en una Central de Ciclo Combinado de más de 6 años (2006-2011), proporcionado por la Universidad de California, Irvine en [UCI Machine Learning Repository Combined Cycle Power Plant Data Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)). Para más detalles sobre el conjunto de datos visitar la página de la UCI, o las siguientes referencias:\n\n* Pinar Tufekci, [Prediction of full load electrical power output of a base load operated combined cycle power plant using machine learning methods](http://www.journals.elsevier.com/international-journal-of-electrical-power-and-energy-systems/), International Journal of Electrical Power & Energy Systems, Volume 60, September 2014, Pages 126-140, ISSN 0142-0615.\n* Heysem Kaya, Pinar Tufekci and Fikret S. Gurgen: [Local and Global Learning Methods for Predicting Power of a Combined Gas & Steam Turbine](http://www.cmpe.boun.edu.tr/~kaya/kaya2012gasturbine.pdf), Proceedings of the International Conference on Emerging Trends in Computer and Electronics Engineering ICETCEE 2012, pp. 13-18 (Mar. 2012, Dubai)."],"metadata":{"id":"cZOu_TG4bzil","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2fab5087-5beb-4801-896a-9125acc097e0"}}},{"cell_type":"code","source":["# Descomprimimos la carpeta power.zip\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1N2vCfPrb1n_","outputId":"3391ab8b-e026-4550-9262-5ac5114da30b","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6d7bb09-c14c-4be8-9b4f-a057e2589255"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Tarea a realizar durante la primera parte:**\n\nRevisar la documentacion y referencias de:\n* [Spark Machine Learning Pipeline](https://spark.apache.org/docs/latest/ml-guide.html#main-concepts-in-pipelines)."],"metadata":{"id":"Ir6LowQodgDU","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01679da7-1b33-4bd2-9f4e-c92d833ed38d"}}},{"cell_type":"markdown","source":["**Tarea a realizar durante la primera parte:**Revisar la documentacion y referencias de:\n* [Spark Machine Learning Pipeline](https://spark.apache.org/docs/latest/ml-guide.html#main-concepts-in-pipelines).\n19:06\n\n## Parte 1: Extracción, transformación y carga [ETL] del dataset\n\nAhora que entendemos lo que estamos tratando de hacer, el primer paso consiste en cargar los datos en un formato que podemos consultar y utilizar fácilmente. Esto se conoce como ETL o \"extracción, transformación y carga\". Primero, vamos a cargar nuestro archivo de HDFS.Nuestros datos están disponibles en la siguiente ruta:\n\n```\n/carpeta-datos/pra2\n```"],"metadata":{"id":"GIQ1rzbQeKll","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b413ea5-2fae-4e20-9d36-d104186c2849"}}},{"cell_type":"markdown","source":["### Ejercicio 1(a)\n\nEmpezaremos por visualizar una muestra de los datos. Para esto usaremos las funciones de hdfs para explorar el contenido del directorio de trabajo:/carpeta/datos/pra2"],"metadata":{"id":"Z4S0KC8bekNS","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2d957dd-330c-46cb-b4d5-8a0c493b87d7"}}},{"cell_type":"code","source":["!hdfs dfs -ls /nombre-carpeta/data/pra2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R4PSHpQJepVG","outputId":"214a6d54-f8bf-4feb-dbca-26178178cf94","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"927b659c-e83c-4522-b16f-218fac41101b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/bin/bash: hdfs: command not found\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/bin/bash: hdfs: command not found\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Usar la función `cat` y `| head -10` para visualizar el contenido de las primeras 10 filas del primer fichero de la lista"],"metadata":{"id":"Y-rgbydFe5r6","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dbd5850d-a765-4f6c-acf6-57a4e0fb9609"}}},{"cell_type":"code","source":["!hdfs dfs -cat /content/pra2/sheet1.csv | head -10"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_DBXI9YTe6WU","outputId":"6f7bface-18f5-48c7-c430-d5055e6dce71","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d516692-c1ea-40aa-b60c-139d67c585a3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/bin/bash: hdfs: command not found\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/bin/bash: hdfs: command not found\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["==========================================================================================================================================================================================================\n### Ejercicio 1(b)\n\nAhora usaremos PySpark para visualizar las 5 primeras líneas de los datos\n\n*Hint*: Primero crea un RDD a partir de los datos usando [`sc.textFile()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.textFile).\n\n*Hint*: Luego piensa como usar el RDD creado para mostrar datos, el método [`take()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take) puede ser una buena opción a considerar."],"metadata":{"id":"obGoTIeyfNCT","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6bd70edf-c306-4ca1-9152-521d74939c23"}}},{"cell_type":"code","source":["# En modo cluster\n"],"metadata":{"id":"Aw7VUWnafVIN","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60bbf860-67aa-4fe1-bbcf-fa2db910e1c6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Cargamos los ficheros y guardamos la variable\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99bDU9POfuCR","outputId":"c7c09672-19f2-4a93-b401-716f7e4f6e39","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf63f517-6a98-469c-98cf-5dfa5a0d516f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# guardarmos el resultado\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YfF29tysgaQj","outputId":"b8c7ed28-c090-4da4-cf6b-e78b3922070a","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0707aad-4a64-4909-a3c4-9c355273926e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#data.take(20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RmWbcj9_gzO1","outputId":"239ce73b-bd88-469c-ec1f-469d90161b3e","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"238e366b-5f94-41ec-8135-0d70abce30c8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["A partir nuestra exploración inicial de una muestra de los datos, podemos hacer varias observaciones sobre el proceso de ETL:\n- Los datos son un conjunto de .csv (archivos con valores separados por coma) \n- Hay una fila de cabecera, que es el nombre de las columnas\n- Parece que el tipo de los datos en cada columna es constante (es decir, cada columna es de tipo double)\n\nEl esquema de datos que hemos obtenido de UCI es:\n- AT = Atmospheric Temperature in C\n- V = Exhaust Vacuum Speed\n- AP = Atmospheric Pressure\n- RH = Relative Humidity\n- PE = Power Output.  Esta es la variable dependiente que queremos predecir usando los otras cuatro\n\nPara usar el paquete Spark CSV, usaremos el método [sqlContext.read.format()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.format) para especificar el formato de la fuente de datos de entrada: `'csv'`\n\nPodemos especificar diferentes opciones de como importar los datos usando el método [options()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.options).\n\nUsaremos las siguientes opciones:\n- `delimiter=','` porque nuestros datos se encuentran delimitados por comas\n- `header='true'` porque nuestro dataset tiene una fila que representa la cabecera de los datos\n- `inferschema='true'` porque creemos que todos los datos son números reales, por lo tanto la librería puede inferir el tipo de cada columna de forma automática.\n\nEl ultimo componente necesario para crear un DataFrame es determinar la ubicación de los datos usando el método [load()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.load).\n\nJuntando todo, usaremos la siguiente operación:\n\n`sqlContext.read.format().options().load()`"],"metadata":{"id":"epTOxfV6hmyH","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cb068ac-38ac-4f14-83ac-36e04543b423"}}},{"cell_type":"markdown","source":["### Ejercicio 1(c)\n\nCrear un DataFrame a partir de los datos.\n- El formato es csv\n\nEn el campo opciones incluiremos 3, formadas por nombre de opción y valor, separadas por coma.\n- El separador es el tabulador\n- El fichero contiene cabecera 'header'\n- Para crear un dataframe necesitamos un esquema (schema). A partir de los datos Spark puede tratar de inferir el esquema, le diremos 'true'.\n\nEl directorio a cargar es el especificado anteriormente. Es importante indicarle a Spark que es una ubicación ya montada en el sistema dbfs, como se ha mostrado en el ejercicio 2a."],"metadata":{"id":"1ox4Ckbbh_zo","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4173360-0178-4daf-bb34-83db882370f8"}}},{"cell_type":"code","source":["from pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)"],"metadata":{"id":"MCwZ-nZAiAre","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fcf9d3e-ca18-40b6-9dc1-d9eff2e07280"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/databricks/spark/python/pyspark/sql/context.py:82: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/databricks/spark/python/pyspark/sql/context.py:82: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Creamos el dataframe\n"],"metadata":{"id":"ss6qrDG7iWxa","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f9b1e9a-0a0d-4305-964a-109fe659cd6e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#type(powerPlantDF)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h42tIWO1jLy2","outputId":"e9e6be3b-d2de-4ac2-fe5c-1f890210ffa9","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"efcae546-3936-43d0-8581-1f94942406b4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Contamos cuantos datos tenemos en nuestro dataframe\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ftVOf5zjTmQ","outputId":"7478ae39-7614-41cc-cd20-515121f18a16","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46003a33-007d-430d-b5d2-40d0c4a20aeb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# TEST\n# expected = set([(s, 'double') for s in ('AP', 'AT', 'PE', 'RH', 'V')])\n# assert expected==set(powerPlantDF.dtypes), \"Incorrect schema for powerPlantDF\""],"metadata":{"id":"xUZ04D2gkH55","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9878a49-0538-429f-af3d-d7b5267a71a0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Comprobamos los tipos de valores\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A4eXelfykPX6","outputId":"f999d9e6-d24e-4528-ad90-a0eba1180d07","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67bfca2d-4ebf-4113-ac4d-d7bdd10e6661"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Para mostrar el dataframe\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NMTtQO49kemW","outputId":"209b3d36-b4a4-4413-f830-595b7c06a0fe","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc859039-d1e7-4448-ac32-9c6dcd76c19e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# mostramos los 10 primeros resultados o show() los primeros 20\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yMGTl1_GkpLO","outputId":"89ccdb18-45b8-4209-dbed-fba6c4993a17","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0d867d7-bd9b-43f1-87e2-e0e82fec8411"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Ahora en lugar de usar [spark csv](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html) para inferir (inferSchema()) los tipos de las columnas, especificaremos el esquema como [DataType](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.DataType), el cual es una lista de [StructField](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.StructType).\n\nLa lista completa de tipos se encuentra en el modulo [pyspark.sql.types](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types). Para nuestros datos, usaremos [DoubleType()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.DoubleType).\n\nPor ejemplo, para especificar cual es el nombre de la columna usaremos: `StructField(`_name_`,` _type_`, True)`. (El tercer parámetro, `True`, significa que permitimos que la columna tenga valores null.)\n\n### Ejercicio 1(d)\n\nCrea un esquema a medida para el dataset."],"metadata":{"id":"WSbfe0xqh_mz","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4ca53a8-f255-4342-b6da-bfb1f4934664"}}},{"cell_type":"code","source":["# para crear un nuevo schema importamos los métodos de los tipos\nfrom pyspark.sql.types import *\n\n# construimos el schema\n"],"metadata":{"id":"f0J5KMotlK8h","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26f9cb1d-85e7-41b3-b2e6-f8f89438a468"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# TEST\n# assert set([f.name for f in customSchema.fields])==set(['AT', 'V', 'AP', 'RH', 'PE']), 'Incorrect column names in schema.'\n# assert set([f.dataType for f in customSchema.fields])==set([DoubleType(), DoubleType(), DoubleType(), DoubleType(), DoubleType()]), 'Incorrect column types in schema.'"],"metadata":{"id":"n8eTWHjamWa4","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a39003a7-81fd-4f7f-b819-49f26081ed40"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Exercicio 1(e)\n\nAhora, usaremos el esquema que acabamos de crear para leer los datos. Para realizar esta operación, modificaremos el paso anterior `sqlContext.read.format`. Podemos especificar el esquema haciendo:\n- Anadir `schema = customSchema` al método load (simplemente anadelo usando una coma justo después del nombre del archivo)\n- Eliminado la opción `inferschema='true'` ya que ahora especificamos el esquema que han de seguir los datos"],"metadata":{"id":"t18J4tXXmdzH","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e210efeb-7908-4e99-bc05-a0ad459dbb56"}}},{"cell_type":"code","source":["# creamos el dataframe con un customSchema\n"],"metadata":{"id":"Dz0hokFhmeby","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c167dd8b-8db2-4011-b1a4-33ffba581996"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#alt_powerPlantDF.show(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lKSAYIjlm-VB","outputId":"706e7869-d0d1-4133-e865-397ab7dde259","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aea43f38-84d0-45fe-ba5e-479676306ac2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# print(alt_powerPlantDF.dtypes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3iwfczTAmXvI","outputId":"97e61dfe-6072-4849-d0b1-4aec315de869","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a9b3dfb-f9bc-4946-aa79-0ba842e13900"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Parte 2: Explorar tus Datos\n\n### Ejercicio 2(a)\n\n​\n\nAhora que ya hemos cargado los datos, el siguiente paso es explorarlos y realizar algunos análisis y visualizaciones básicas.\n\n​\n\nEste es un paso que siempre se debe realizar **antes de** intentar ajustar un modelo a los datos, ya que este paso muchas veces nos permitirá conocer una gran información sobre los datos.\n\nEn primer lugar vamos a registrar nuestro DataFrame como una tabla de SQL llamado power_plant. Debido a que es posible que repitas esta práctica varias veces, vamos a tomar la precaución de eliminar cualquier tabla existente en primer lugar.\n\nUna vez ejecutado el paso anterior, podemos registrar nuestro DataFrame como una tabla de SQL usando sqlContext.registerDataFrameAsTable().\n\nCrea una tabla llamada power_plant con las indicaciones mostradas."],"metadata":{"id":"fm1wWhuNqtoY","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a24ce11-b478-4569-bb5b-e0ab57472369"}}},{"cell_type":"code","source":[""],"metadata":{"id":"0R6t02DJqufN","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5daeb5de-58ea-44ad-b8dd-252c8f0398d3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Para poder realizar consultas utilizamos queries de SQL\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-JAn1prjrIfM","outputId":"a97e00e6-4baf-4b44-849e-d16a9103f7d5","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"107dd60b-484e-4aa9-a561-ed42179406f7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Ahora que nuestro DataFrame existe como una tabla SQL, podemos explorarlo utilizando comandos SQL y `sqlContext.sql(...)`. Utiliza la función `show()` para visualizar el resultado del dataframe."],"metadata":{"id":"BfFLmhRxrZkK","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4cb64542-b900-45de-8ff9-e68ef10425ef"}}},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Trj5RdqgraFJ","outputId":"0159e349-423c-4808-8f36-5175ae8880a4","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d7bbb8c-2a1f-4820-88de-acd0a2b19445"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Realizamos una prueba con dos columnas\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LlHnRjnOrSZA","outputId":"1a4226d0-f828-4d4c-f750-02d32f4e6dc5","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"adf9b5b3-87db-49e4-84f8-f2325c9f9b87"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AKsJGO9CsNX4","outputId":"66da812d-1a50-4cb6-c22d-fb22036e9195","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"763f113e-2898-441e-bd93-0ffb4b82f7b0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Definición de Esquema**\n\nUna vez más, nuestro esquema es el siguiente:\n\n- AT = Atmospheric Temperature in C\n- V = Exhaust Vacuum Speed\n- AP = Atmospheric Pressure\n- RH = Relative Humidity\n- PE = Power Output\n\nPE es nuestra variable objetivo. Este es el valor que intentamos predecir usando las otras mediciones.\n\n*Referencia [UCI Machine Learning Repository Combined Cycle Power Plant Data Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)*"],"metadata":{"id":"N2wWGWiRsjdo","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c15d850f-dde7-4f1e-bdec-aabda4982949"}}},{"cell_type":"markdown","source":["## 2b\n\nAhora vamos a realizar un análisis estadístico básico de todas las columnas.\n\nCalculad y mostrad los resultados en modo tabla (la función `show()` os puede ser de ayuda):\n* Número de registros en nuestros datos\n* Media de cada columna\n* Máximo y mínimo de cada columna\n* Desviación estándar de cada columna\n\nHint: Revisad [DataFrame](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame) ya que contiene métodos que permiten realizar dichos cálculos de manera sencilla."],"metadata":{"id":"SWp490t9tKtG","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12e1724f-f664-466b-8c0b-a07927d77905"}}},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qpMFKsLQtMtV","outputId":"8b0e8b94-fb8c-4e18-ea46-1d36378c3645","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83f1a9d9-7300-48c5-8db8-ab11005e288d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# df.describe().show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"25YA66HktbgS","outputId":"d31199d1-d4b6-4514-cf8e-809b3d693498","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a256054-7f70-4a7d-ab6f-f80ae123090f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Parte 3: Visualizar los datos\n\nPara entender nuestros datos, intentamos buscar correlaciones entre las diferentes características y sus correspondientes etiquetas. Esto puede ser importante cuando seleccionamos un modelo. Por ejemplo, si una etiqueta y sus características se correlacionan de forma lineal, un modelo de regresión lineal obtendrá un buen rendimiento; por el contrario si la relación es no lineal, modelos más complejos, como arboles de decisión pueden ser una mejor opción. Podemos utilizar herramientas de visualización para observar cada uno de los posibles predictores en relación con la etiqueta como un gráfico de dispersión para ver la correlación entre ellos.\n\n============================================================================\n### Ejercicio 3(a)\n\n#### Añade las siguientes figuras: \nVamos a ver si hay una correlación entre la temperatura y la potencia de salida. Podemos utilizar una consulta SQL para crear una nueva tabla que contenga solo el de temperatura (AT) y potencia (PE), y luego usar un gráfico de dispersión con la temperatura en el eje X y la potencia en el eje Y para visualizar la relación (si la hay) entre la temperatura y la energía.\n\nRealiza los siguientes pasos:\n- Carga una muestra de datos aleatorios de 1000 pares de valores para PE y AT. Puedes utilizar una ordenación aleatoria o un sample() sobre el resultado. Para hacer el plot puedes hacer un collect().\n- Utiliza matplotlib y Pandas para hacer un scatter plot (https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.scatter.html)"],"metadata":{"id":"xuK-xyvatxm9","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9104732b-f48a-4ea9-b372-e9abd9fb54cd"}}},{"cell_type":"code","source":["from matplotlib import pyplot as plt\nimport pandas as pd\nfrom pyspark.sql.functions import rand\n"],"metadata":{"id":"cRjy6o7qt4KN","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57ef6101-c304-4fce-bb5d-81f0c2eb9f65"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GqDfOKAHuyXa","outputId":"e9cef98d-cb03-4e9c-e5a8-ca97b74039d8","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14005573-a60d-484e-8e03-e3191fd41e0a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qs43clcAu2el","outputId":"c7179ab7-6849-4f2e-8b26-3279312d10bf","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6df6faf-09e5-43bc-ac14-bc06f93d8cd2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"id":"-S1fgrDGuklB","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10b43113-6d4d-4dba-95bb-95fe7f3c5d6a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"2Ls0C2azutyV","outputId":"4cf2db23-d5cf-48cd-b22d-3999bb013b6f","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bbcaa45a-77b7-4d46-89cc-4880341f12b1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# creamos un gráfico tipo scatterplot\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"MHROaWMUthtm","outputId":"5e94d69a-4390-43ca-99b9-e9bee8188b1f","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4535757d-f721-40b0-9b8d-6c5fbf964b94"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Ejercicio 3(b)\n\nRepitiendo el proceso anterior, usa una sentencia SQL para crear un gráfico de dispersión entre las variables Power (PE) y Exhaust Vacuum Speed (V)."],"metadata":{"id":"TABQ8mjVw9ID","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e81d3ee-a6e3-43b6-af3e-ea0e00327cd7"}}},{"cell_type":"code","source":["\n#OPCIONES DE MATPLOTLIB - Titulo, x-y-label...\n# plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"CintSFUtw9vm","outputId":"dda6b835-8615-4439-8d6a-1f0d5455fd70","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58d8ebec-348c-4160-9214-fb42e0530734"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Ahora vamos a repetir este ejercicio con el resto de variables y la etiqueta Power Output.\n\n### Ejercicio 3(c)\n\nUsa una sentencia SQL para crear un gráfico de dispersión entre las variables Power (PE) y Pressure (AP)."],"metadata":{"id":"CAtDEVTo1ru0","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"999c532c-664c-4565-b1cd-8e0fae2e9eb1"}}},{"cell_type":"code","source":["\n#OPCIONES DE MATPLOTLIB - Titulo, x-y-label...\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"9s1irroA1uio","outputId":"9d942ac0-c156-490f-aa02-19719438f756","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cc23f4d-8fbb-46ef-ac29-3b49085f79cc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Ejercicio 3(d)\n\nUsa una sentencia SQL para crear un gráfico de dispersión entre las variables Power (PE) y Humidity (RH)."],"metadata":{"id":"Godaz0Q6xGh7","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08a12429-e209-4a0d-9025-cfa871388ad3"}}},{"cell_type":"code","source":["\n#OPCIONES DE MATPLOTLIB - Titulo, x-y-label...\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"zRZ4U1QlxHWq","outputId":"bf4ce6b7-e693-489f-9468-272515f64476","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc10284a-587a-4830-8dd1-d7ceef1c22d6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Parte 4: Preparación de los datos\n\nEl siguiente paso es preparar los datos para aplicar la regresión. Dado que todo el dataset es numérico y consistente, esta será una tarea sencilla y directa.\n\nEl objetivo es utilizar el método de regresión para determinar una función que nos de la potencia de salida como una función de un conjunto de características de predicción. El primer paso en la construcción de nuestra regresión es convertir las características de predicción de nuestro DataFrame a un vector de características utilizando el método [pyspark.ml.feature.VectorAssembler()](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler).\n\nEl VectorAssembler es una transformación que combina una lista dada de columnas en un único vector. Esta transformación es muy útil cuando queremos combinar características en crudo de los datos con otras generadas al aplicar diferentes funciones sobre los datos en un único vector de características. Para integrar en un único vector toda esta información antes de ejecutar un algoritmo de aprendizaje automático, el VectorAssembler toma una lista con los nombres de las columnas de entrada (lista de strings) y el nombre de la columna de salida (string)."],"metadata":{"id":"qhlLtyxbiKFu","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"545c26a7-5e90-40e9-8579-4de1398d00e2"}}},{"cell_type":"markdown","source":["============\n### Ejercicio 4\n\n- Leer la documentación y los ejemplos de uso de [VectorAssembler](https://spark.apache.org/docs/latest/ml-features.html#vectorassembler)\n- Convertir la tabla SQL `power_plant` en un `dataset` llamado datasetDF\n- Establecer las columnas de entrada del VectorAssember: `[\"AT\", \"V\", \"AP\", \"RH\"]`\n- Establecer la columnas de salida como `\"features\"`"],"metadata":{"id":"nSCUNERKjth1","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb8417e9-4bb2-4b47-8458-63a2d7c07c2a"}}},{"cell_type":"markdown","source":["En entorno **BATCH** podemos utilizar tanto Scikit-Learn con dataframe de Pandas o seguir con SparkML o MLlib (esta práctica utilizaremos ML de Spark).\n\nNOTA: En **STREAMING**, siempre es aconsejable utilizar ML en streaming (Spark Streaming ML, Apache Mahout, MLflow...) mejor no utilizar scikit-Learn"],"metadata":{"id":"se2FaprEpbOv","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d4bd66a-60ac-4c5d-a15c-f983f57a7543"}}},{"cell_type":"code","source":["# creamos el dataset para vectorizarlo\nfrom pyspark.ml.feature import VectorAssembler\n\n"],"metadata":{"id":"2-E9wzc8o1eY","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23ffbd06-68b3-4cad-92f7-a55445da6a8b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Creamos las columnas INPUT\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ci2UYZxZqr7l","outputId":"f21ea4b3-7af8-42db-ec40-752f21343c41","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7007c31f-b9e2-4515-ad47-1d1d88366351"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# aplicamos el vectorizer al dataframe\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g3y8_CNorrQa","outputId":"66c83682-3fa3-4624-bac4-2fd8f271cdfc","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56bde7bf-d06c-4f35-aef7-3718b711694d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Parte 5: Modelar los datos\n\nAhora vamos a modelar nuestros datos para predecir que potencia de salida se dara cuando tenemos una serie de lecturas de los sensores\n\nLa API de [Apache Spark MLlib](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml) ofrece diferentes implementaciones de técnicas de regresion para modelar datasets. Ene ste ejercicio vamos a modelar nuestros datos para predecir que potencia de salida se dara cuando tenemos una serie de lecturas de los sensores basándonos en una simple regresion lineal ya que vimos algunos patrones lineales en nuestros datos en los graficos de dispersion durante la etapa de exploracion.\n\nNecesitamos una forma de evaluar como de bien nuestro modelo de [regresion lineal](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression) predice la produccion de potencia en funcion de parametros de entrada. Podemos hacer esto mediante la division de nuestros datos iniciales establecidos en un _Training set_ utilizado para entrenar a nuestro modelo y un _Test set_ utilizado para evaluar el rendimiento de nuestro modelo. Podemos usar el metodo nativo de los DataFrames [randomSplit()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) para dividir nuestro dataset. El metodo toma una lista de pesos y una semilla aleatoria opcional. La semilla se utiliza para inicializar el generador de numeros aleatorios utilizado por la funcion de division.\n\nNOTA: Animamos a los alumnos a explorar las diferentes técnicas de regresión disponibles en la [API ML de Spark](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.classification)"],"metadata":{"id":"wU5iwpIXtWID","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c563f286-51b2-4ab6-84e2-45c96d818cdb"}}},{"cell_type":"markdown","source":["================================================================================\n### Ejercicio 5(a)\n\nUtiliza el método [randomSplit()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) para dividir `datasetDF` en trainingSetDF (80% del DataFrame de entrada) y testSetDF (20% del DataFrame de entrada), para poder reproducir siempre el mismo resultado, usar la semilla 1800009193. Finalmente, cachea (cache()) cada datafrane en memoria para maximizar el rendimiento."],"metadata":{"id":"774WV1xhtjev","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e55c8620-c25c-4e70-af37-828ac861f267"}}},{"cell_type":"code","source":["# creamos train y test\nfrom pyspark.storagelevel import StorageLevel\n\n# creamos una semilla para la aleatoriedad del split\n"],"metadata":{"id":"yG6chR4ytnNb","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d95a0f7-9e1d-4e09-8f86-a37f661da9a7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Guardamos los split en cache\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NHmJP1mOv46h","outputId":"9b538307-ef41-49d4-c4d2-d73ea9428e62","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee4165a9-1fb7-4b5f-9d6f-7ecd0096e436"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yA_iV6sIwaRJ","outputId":"59537bde-d066-4302-d7ea-e8587ef8659b","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d28f0a5-3118-4494-a9c0-07bbc1783307"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pUVLsYWBwdjK","outputId":"6b0cf1b6-67d7-4d09-c6b3-c924c76cf6a4","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f277633c-d277-450c-a5ee-02d2c1fd683b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# guardamos en variables\n"],"metadata":{"id":"7NPRnNC1wfW2","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c06a1f19-b226-4539-b4cf-0b4c165c741f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["A continuacion vamos a crear un modelo de regresion lineal y utilizar su ayda para entender como entrenarlo. Ver la API de [Linear Regression](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression) para mas detalles.\n\n### Ejercicio 5(b)\n\n- Lee la documentacion y los ejemplos de [Linear Regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-regression)\n- Ejecuta la siguiente celda"],"metadata":{"id":"P4dqkSdRx3wZ","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd93f37f-65f5-470e-8500-532a0e851881"}}},{"cell_type":"code","source":["from pyspark.ml.regression import LinearRegression\nfrom pyspark.ml import Pipeline"],"metadata":{"id":"gRf-vauBx4hj","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2491740-f422-4eac-a363-59fe35981bb0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# inicializamos el estimador\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j7sDc7EV72JW","outputId":"8701a751-cd36-4f5d-a5bd-a5d43eefc96e","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99395f89-ceaf-45c9-9cd1-d2d5c419fb6e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["La siguiente celda esta basada en [Spark ML Pipeline API for Linear Regression](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression).\n\nEl primer paso es establecer los valores de los parametros siguientes:\n- Define el nombre de la columna a donde guardaremos la prediccion como \"Predicted_PE\"\n- Define el nombre de la columna que contiene la etiqueta como \"PE\"\n- Define el numero maximo de iteraciones a 100\n- Define el parametro de regularizacion a 0.1\n\nAhora, crearemos el [ML Pipeline](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Pipeline) (flujo de ejecucion) y estableceremos las fases del pipeline como vectorizar y posteriormente aplicar el regresor lineal que hemos definido.\n\nFinalmente, crearemos el modelo entrenandolo con el DataFrame `trainingSetDF`.\n\n### Ejercicio 5(c)\n\n- Lee la documentacion [Linear Regression](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression) documentation\n- Completa y ejecuta la siguiente celda introduciendo los pará,etros descritos para nuestra regresion."],"metadata":{"id":"H91E2HAU9X52","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f74acc5b-7d4f-45b7-b733-2ec7751f651a"}}},{"cell_type":"code","source":["# Para Spark ML configuramos los parámetros de esta manera:\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M3S9sTPs9YnJ","outputId":"3485d9ae-bb10-4b50-a0ab-c0a1751e942d","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab06672d-aba4-40be-a272-b88e9cddd8dd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# utilizamos la API Pipeline para crear el flujo (muy similar al de scikit-learn)\n"],"metadata":{"id":"_PbVendF-ISD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4dc0e526-d6fe-4af0-f647-5d937f9e544e","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"192b8080-ce48-49e3-9777-97be4246c114"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Creamos nuestro primer modelo\n"],"metadata":{"id":"npgUdWuBOegR","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8eb04a6-04ee-4b09-9760-e72ed4017245"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jYrtnOiDOuaB","outputId":"b1c951f4-9a39-40c8-8578-3182eb27300d","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a3ced91-3efd-4d18-85db-3812a1a28ee9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Del articulo de Wikipedia [Linear Regression](https://en.wikipedia.org/wiki/Linear_regression) podemos leer:\n> In statistics, linear regression is an approach for modeling the relationship between a scalar dependent variable \\\\( y \\\\) and one or more explanatory variables (or independent variables) denoted \\\\(X\\\\). In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models.\n\nLos modelos de regresion lineal tienen muchos usos practicos. La mayoria de los cuales se clasifican en de las siguientes dos categorias:\n- Si el objetivo es la prediccion o la reduccion de errores, la regresion lineal puede utilizarse para adaptar un modelo predictivo a un conjunto de datos observados \\\\(y\\\\) y \\\\(X\\\\). Despues de desarrollar un modelo de este tipo, dado un cierto valor  \\\\( X\\\\) del que no conocemos su valor de \\\\(y \\\\), el modelo ajustado se puede utilizarse para hacer una prediccion del valor del posible valor \\\\(y \\\\).\n- Dada una variable \\\\(y\\\\) y un numero de variables \\\\( X_1 \\\\), ..., \\\\( X_p \\\\) que pueden estar relacionadas con \\\\(y\\\\), un analisis de regresion lineal puede ser aplicado a cuantificar como de fuerte es la relacion entre \\\\(y\\\\) y cada \\\\( X_j\\\\), para evaluar que \\\\( X_j \\\\) puede no tener ninguna relacion con \\\\(y\\\\), y de esta forma identificar que subconjuntos de \\\\( X_j \\\\) contienen informacion redundante sobre \\\\(y\\\\).\n\nComo estamos interesados en ambos usos, nos gustaria para predecir la potencia de salida en funcion de las variables de entrada, y nos gustaria saber cuales de las variables de entrada estan debilmente o fuertemente correlacionadas con la potencia de salida.\n\nYa que una regresion lineal tan solo calcula la linea que minimiza el error cuadratico medio en el dataset de entrenamiento, dadas multiples dimensiones de entrada podemos expresar cada predictor como una funcion lineal en la forma:\n\n\\\\[ y = a + b x_1 + b x_2 + b x_i ... \\\\]\n\ndonde \\\\(a\\\\) es el intercept (valor para el punto 0) y las \\\\(b\\\\) son los coeficientes.\n\nPara expresar los coeficientes de esa linea podemos recuperar la etapa del Estimador del Modelo del pipeline y de expresar los pesos y el intercept de la funcion.\n\n### Ejercicio 5(d)\n\nEjecuta la celda siguiente y asegurate que entiendes lo que sucede."],"metadata":{"id":"3Ecs5ylfO6F2","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7d66aba-eb8f-4c9b-ab04-0708b064a10d"}}},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vfw2dWLHS_as","outputId":"b1eef0d4-ce24-4a0d-a811-60830735a310","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8171f885-fa2b-4a6e-899d-f51ea1981a48"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# creamos la intercept\n\n\n# Creamos los coeficientes\n\n\n# crear un array con el nombre de los atributos excepto el PE (nuestra var dep)\n\n\n# Unimos los coeficientes y los atributos\n\n\n# creamos la ecuación de la Regresión Lineal\n\n\n# Finally here is our equation\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EECf4jnKO7HU","outputId":"380e1d49-b927-46f6-f858-062d1978b467","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc3e7960-3a25-4453-8816-1fa75e4eed02"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### ejemplo resultado\n\nLinear Regression Equation: y = 436.42968944499756 - (1.9177667442995632 * AT) - (0.2541937108619571 * V) + (0.07919159694384864 * AP) - (0.1473348449135295 * RH)"],"metadata":{"id":"0ORHDJGvXOpp","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05901264-7468-4383-82a4-7915b027dc91"}}},{"cell_type":"markdown","source":["### PREGUNTA\n\n- Que obtenemos como resultado de esta ejecución?\n\nLa ecuación devuelve nuestra observación meno el coeficiente positivo o negativo por cada uno de los atributos o variables indipendientes (predictores), cuanto más pequeños y se acerca a nuestra observación, el valor predicho es casi a cero, significa que la predicción se acerca a nuestra intercepta, cuando más lejano se encuentra (positivo o negativo), significa que el valor predicho se encuentra lejos a nivel de distancia de nuestra intercepta. Hay que examinar entre ellos los valores residuales, ya que valores muy ajustados produciría un **overfitting** y muy lejos **underfitting**."],"metadata":{"id":"urBACmakXtIq","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c4867e0-859a-45d1-9061-d23a5748dc62"}}},{"cell_type":"markdown","source":["***\n\n### Ejercicio 5(e)\n\nAhora estudiaremos como se comportan nuestras predicciones en este modelo. Aplicamos nuestro modelo de regresion lineal para el 20% de los datos que hemos separado del conjunto de datos de entrada. La salida del modelo sera una columna de produccion de electricidad teorica llamada \"Predicted_PE\".\n\n- Ejecuta la siguiente celda\n- Desplazate por la tabla de resultados y observa como los valores de la columna de salida de corriente (PE) se comparan con los valores correspondientes en la salida de potencia predecida  (Predicted_PE)"],"metadata":{"id":"tU88j44mbUiJ","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1d34365-bb64-4f1c-bc74-6a4cf506f50a"}}},{"cell_type":"code","source":["# aplicamos el LRmodel a nuestro test data\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"4qjzOU7fbVFa","outputId":"227d271b-90dd-4add-ecef-3dc9f1ba6141","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8127586a-3e0b-4662-ae62-a373a9fb55cb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["A partir de una inspección visual de las predicciones, podemos ver que están cerca de los valores reales.\n\nSin embargo, nos gustaría disponer de una medida científica exacta de la bondad del modelo. Para realizar esta medición, podemos utilizar una métrica de evaluación como la [Error cuadrático medio](https://en.wikipedia.org/wiki/Root-mean-square_deviation) (RMSE) para validar nuestro modelo.\n\nRSME se define como: \\\\( RMSE = \\sqrt{\\frac{\\sum_{i = 1}^{n} (x_i - y_i)^2}{n}}\\\\) donde \\\\(y_i\\\\) es el valor observado \\\\(x_i\\\\) es el valor predicho\n\nRMSE es una medida muy habitual para calcular las diferencias entre los valores predichos por un modelo o un estimador y los valores realmente observados. Cuanto menor sea el RMSE, mejor será nuestro modelo.\n\nSpark ML Pipeline proporciona diferentes métricas para evaluar modelos de regresión, incluyendo [RegressionEvaluator()](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator).\n\nDespués de crear una instancia de [RegressionEvaluator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator), fijaremos el nombre de la columna objetivo \"PE\" y  el nombre de la columna de predicción a \"Predicted_PE\". A continuación, invocaremos el evaluador en las predicciones.\n\n### Ejercicio 5(f)\nCompleta y ejecuta la celda siguiente:"],"metadata":{"id":"RuopUWaTdXj-","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"021fde61-689b-4389-a21c-5dc6d2163abc"}}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n\n# creamos nuestra métrica de evaluación RMSE\n"],"metadata":{"id":"2orr8v-XdYZt","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b90a3cff-0a0a-4d59-a9ae-6cf0b58a1538"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# inicializamos nuestro evaluator\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pdeDOmpLeNDG","outputId":"ef41bbb7-eba7-48da-de2b-8e4a276745d6","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77a1f76f-1a02-4666-ad50-6866ef598e63"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Otra medida de evaluación estadística muy útil es el coeficiente de determinación, que se denota \\\\(R ^ 2 \\\\) o \\\\(r ^ 2\\\\) y pronunciado \"R cuadrado\". Es un número que indica la proporción de la variación en la variable dependiente que es predecible a partir de las variables independientes y proporciona una medida de lo bien que los resultados observados son replicados por el modelo, basado en la proporción de la variación total de los resultados explicada por el modelo. El coeficiente de determinación va de 0 a 1 (más cerca de 1), y cuanto mayor sea el valor, mejor es nuestro modelo.\n\n\nPara calcular \\\\(r^2\\\\), hemos de ejecutar el evaluador `regEval.metricName: \"r2\"`\n\nVamos a calcularlo ejecutando la celda siguiente."],"metadata":{"id":"ygtvoTHZfGmA","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cea1b4c5-abfe-4b63-bd5f-c636389df689"}}},{"cell_type":"code","source":["# Utilizamos la otra métrica de evaluación\n#r2 = regEval.evaluate(predictions, {regEval.metricName: \"r2\"})\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0keav9vvfHDf","outputId":"ac9f2073-1309-4ffb-9b03-de50ada673c8","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91c57926-9d5f-4fd3-b913-9a7995918f08"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["- Que resultado de \\\\(r^2\\\\) hemos obtenido? A partir de dicho parametro, crees que el modelo calculado se ajusta bien a los datos?\n\n### RESPUESTA\nEl R2 indica la proporción de la varianza entre la variable dependiente y los atributos indipendientes. Al variar entre 0 y 1 cuanto más cercano esté al 1 es mejor, que corresponde al 100% es decir 1. En este caso tenemos 0.93, al parecer es un buen indicador.\n\n***"],"metadata":{"id":"4Zpp39l_ffkZ","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05de7114-2e0f-473d-b3db-d6ff68054469"}}},{"cell_type":"markdown","source":["En general, suponiendo una distribución Gaussiana de errores, un buen modelo tendrá 68% de las predicciones dentro de 1 RMSE y 95% dentro de 2 RMSE del valor real (ver http://statweb.stanford.edu/~susan/courses/s60/split/node60.html).\n\nVamos a examinar las predicciones y ver si un RMSE como el obtenido cumple este criterio.\n\nCrearemos un nuevo DataFrame usando [selectExpr()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.selectExpr) para generar un conjunto de expresiones SQL, y registrar el DataFrame como una tabla de SQL utilizando [registerTempTable()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.registerTempTable).\n\n### Ejercicio 5(g)\n\nEjecuta la celda siguiente y asegúrate que entiendes lo que sucede."],"metadata":{"id":"MEiTPpgKge0F","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4fe6568a-89d5-4c0c-94f0-0511aa433f85"}}},{"cell_type":"code","source":["# calculamos los errores residuales y dividirlo por el RMSE\n"],"metadata":{"id":"k6YAO4kVgff4","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f716d90-8991-40bb-aa23-6e132d0a5b25"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Podemos utilizar sentencias SQL para explorar la tabla `Power_Plant_RMSE_Evaluation`. En primer lugar vamos a ver qué datos en la tabla utilizando una sentencia SELECT de SQL.\n\nCompleta la siguiente consulta para que devuelva los elementos de la tabla `Power_Plant_RMSE_Evaluation` y muestra algunos por pantalla con la acción show()."],"metadata":{"id":"XC-n-2JQgzO-","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0665abfa-4c08-4c18-bc77-5cd8ab85c6f1"}}},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_tR5dy0Rgzxx","outputId":"28f1b972-271a-4ae3-a303-9e6119ce46e7","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0f76738-d259-4a51-bdbd-e297d2adb684"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## TAREA PARA CASA - parte 1\n\n## Responde las siguientes preguntas\n- ¿A partir de los resultados del ejercicio 5, te parece que el modelo ajusta bien los datos?\n- ¿Qué otras técnicas de regresión usaríais a partir del análisis de datos del apartado 3? ¿Estan todas disponibles en Spark? ¿Y si no es el caso, qué otros librarias y herramientas de Machine Learning podriamos usar?"],"metadata":{"id":"0GCUt45aiuln","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9a721df-41a9-42b7-b8d3-47140fcd0ebd"}}},{"cell_type":"markdown","source":["### Ejercicio 5(h)Mostrad el RMSE como un histograma.\nhttps://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.hist.htmlPuedes jugar con los diferentes parámetros del histograma como el numero de 'bins' o el parámetro densidad, que cambia entre el 'contaje' de elementos en cada bin o la normalización para que el area del histograma sea 1.\nDe forma similar al ejercicio 3, toma una muestra aleatoria de 2000 elementos y haz un collect para hacer el plot."],"metadata":{"id":"A6gJchfqhruC","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f50044e5-8c95-4983-ba8d-42748b60beea"}}},{"cell_type":"code","source":["import numpy as np\n\n\n#MATPLOTLIB \n#LO PODÉIS MODIFICAR PARA QUE PODÁIS MOSTRAR LA DISTRIBUCIÓN EN HISTOGRAMA DE RSME\n#ESTO ES SOLO UNA BASE, PERO HAY MANERAS DIFERENTES\n\n"],"metadata":{"id":"kJs1-HtNh2IG","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5ddc52f-32b3-4544-8722-90d2d252f8ac"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import numpy as np\n\n\n#MATPLOTLIB \n#LO PODÉIS MODIFICAR PARA QUE PODÁIS MOSTRAR LA DISTRIBUCIÓN EN HISTOGRAMA DE RSME\n#ESTO ES SOLO UNA BASE, PERO HAY MANERAS DIFERENTES\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W2IfznbrfgCF","outputId":"6aaba53a-1482-4a3f-d9e7-209a05fdc726","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf15acc3-91d0-4251-9885-ec8ac6c55610"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Observa que el histograma deberia mostrar claramente que el RMSE se centra alrededor de 0 con la gran mayoría de errores dentro de 2 RMSE.\n\nUsando una instrucción SELECT de SQL un poco más compleja, podemos contar el número de predicciones dentro de + o - 1,0 y + o - 2,0.\n\nCuantas predicciones estan dentro de cada uno de los intervalos (+-1 RSME, +-2RSME y más allá)? Completad la parte de código que falta para poder averiguarlo."],"metadata":{"id":"zB8wqOzxiAHb","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8ea0d9a-7d73-4816-b2d6-796d983f39ec"}}},{"cell_type":"code","source":["#Comprobamos la creación del dataframe en Pandas\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"f6cuUJhqfacT","outputId":"e8ab840c-8f82-42c6-fde2-8ddb0548aba6","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd3c7b4a-15d8-4413-95f8-337c08a3418e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\n#MATPLOTLIB "],"metadata":{"id":"1NmZ5TJ4iCF_","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cefb0096-f3b1-41f4-aab0-35b1d22a8bbc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["- ¿Cuantas predicciones estan a 1 RMSE como máximo de los valores reales?\n- ¿Y cuantas predicciones estan a 2 RMSE como máximo de los valores reales?"],"metadata":{"id":"Ir_ZqGlwiDo3","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f908dfd0-6963-4ed4-a814-35b7cf6e171a"}}},{"cell_type":"markdown","source":["## Responde las siguientes preguntas\n1. ¿A partir de los resultados del ejercicio 5, te parece que el modelo ajusta bien los datos?\n2. ¿Qué otras técnicas de regresión usaríais a partir del análisis de datos del apartado 3? ¿Estan todas disponibles en Spark? ¿Y si no es el caso, qué otros librarias y herramientas de Machine Learning podriamos usar?\n3. Investigad el modelo **LinearRegression** y observad los resultados al cambiar el resto de parámetros\n4. Pueden ir investigando el resto de estimadores y observar y comparar los resultados"],"metadata":{"id":"V-MvBB-giFSJ","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c6ae594-4c83-48c8-b66e-6621decaf41d"}}}],"metadata":{"colab":{"name":"02_Spark_Standalone_en_Google_Colab_022021_SparkML","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"application/vnd.databricks.v1+notebook":{"notebookName":"02_Forecast_Demanda_consumo_energitico_SparkML_alumni","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3268296991063657}},"nbformat":4,"nbformat_minor":0}
